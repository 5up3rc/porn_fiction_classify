{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文件导入与处理\n",
    "\n",
    "```\n",
    "author: yudake\n",
    "date: 2/18/2018\n",
    "```\n",
    "\n",
    "本项目是利用 **文本卷积神经网络（TextCNN）** 对文章进行二分类，以区别文章是否为色情小说。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、导入将要使用的库\n",
    "\n",
    "- numpy：Anconda环境下自带\n",
    "- sklearn.utils.shuffle：Anconda环境下自带\n",
    "- datetime：Anconda环境下自带\n",
    "- sklearn.model_selection.train_test_split：Anconda环境下，高版本sklearn自带\n",
    "- tensorflow：用来进行分词，需自行安装\n",
    "- pickle：用来进行存储处理后的数据，需自行安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、分割训练集与测试集\n",
    "\n",
    "数据中后2000条为测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23747, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pickle.load(open('data_processed/data_processed.p', mode='rb'))\n",
    "word_set = pickle.load(open('data_processed/word_set.p', mode='rb'))\n",
    "data.drop(0, axis=0, inplace=True)\n",
    "data['label'] = data['label'].astype('float32')\n",
    "data = shuffle(data)  # 打乱数据顺序\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = data[0:21747]\n",
    "test = data[21747:23747]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、神经网络模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_set_size = len(word_set)\n",
    "sentences_size = 1500\n",
    "  \n",
    "num_epochs = 3\n",
    "batch_size = 16\n",
    "learningrate = 0.005\n",
    "  \n",
    "window_sizes = {2, 3, 4, 5}\n",
    "filter_num = 2\n",
    "embed_dim = 32\n",
    "\n",
    "save_dir = './save/save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        # 计算参数的均值，并使用tf.summary.scaler记录\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        \n",
    "        # 计算参数的标准差\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            # 使用tf.summary.scaler记录记录下标准差，最大值，最小值\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            # 用直方图记录参数的分布\n",
    "            tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_cnn(fiction, dropout_keep_prob):\n",
    "    # 嵌入层\n",
    "    with tf.name_scope('word_embedding'):\n",
    "        fiction_embed_matrix = tf.Variable(tf.random_uniform([word_set_size+1, embed_dim], -1, 1), name = \"fiction_embed_matrix\")\n",
    "        fiction_embed_layer = tf.nn.embedding_lookup(fiction_embed_matrix, fiction, name = \"fiction_embed_layer\")\n",
    "        fiction_embed_layer_expand = tf.expand_dims(fiction_embed_layer, -1)\n",
    "          \n",
    "    # 卷积池化层\n",
    "    pool_layer_lst = []\n",
    "    for window_size in window_sizes:\n",
    "        with tf.name_scope(\"fiction_conv_maxpool_{}\".format(window_size)):\n",
    "            filter_weights = tf.Variable(tf.truncated_normal([window_size, embed_dim, 1, filter_num],stddev=0.1),name = \"filter_weights\")\n",
    "            variable_summaries(filter_weights)\n",
    "            filter_bias = tf.Variable(tf.constant(0.1, shape=[filter_num]), name=\"filter_bias\")\n",
    "            variable_summaries(filter_bias)\n",
    "              \n",
    "            conv_layer = tf.nn.conv2d(fiction_embed_layer_expand, filter_weights, [1,1,1,1], padding=\"VALID\", name=\"conv_layer\")\n",
    "            relu_layer = tf.nn.relu(tf.nn.bias_add(conv_layer,filter_bias), name =\"relu_layer\")\n",
    "              \n",
    "            maxpool_layer = tf.nn.max_pool(relu_layer, [1,sentences_size - window_size + 1 ,1,1], [1,1,1,1], padding=\"VALID\", name=\"maxpool_layer\")\n",
    "            pool_layer_lst.append(maxpool_layer)\n",
    "      \n",
    "    # Dropout层\n",
    "    with tf.name_scope('pool_dropout'):\n",
    "        pool_layer = tf.concat(pool_layer_lst, 3, name =\"pool_layer\")\n",
    "        max_num = len(window_sizes) * filter_num\n",
    "        pool_layer_flat = tf.reshape(pool_layer , [-1, 1, max_num], name = \"pool_layer_flat\")\n",
    "      \n",
    "        dropout_layer = tf.nn.dropout(pool_layer_flat, dropout_keep_prob, name = \"dropout_layer\")\n",
    "          \n",
    "    # softmax层\n",
    "    with tf.name_scope('softmax'):\n",
    "        reshape = tf.reshape(dropout_layer, shape=[batch_size, -1])\n",
    "        dim = reshape.get_shape()[1].value\n",
    "        weights = tf.get_variable('softmax_linear',\n",
    "                                  shape = [8, 2],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                  shape = [2],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.constant_initializer(0.1))\n",
    "        softmax_linear = tf.add(tf.matmul(reshape, weights), biases, name='softmax_linear')\n",
    "        tf.summary.histogram('softmax_linear', softmax_linear)\n",
    "    return softmax_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    fiction = tf.placeholder(dtype=tf.int32, shape=[None, 1500], name='fiction')\n",
    "    targets = tf.placeholder(tf.int32, [None, 1], name=\"targets\")\n",
    "    LearningRate = tf.placeholder(tf.float32, name=\"LearningRate\")\n",
    "    dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "    targets_reshape = tf.reshape(targets, shape=[batch_size])\n",
    "      \n",
    "    # inference\n",
    "    with tf.variable_scope(\"inference\"):\n",
    "        softmax_linear = text_cnn(fiction, dropout_keep_prob)\n",
    "      \n",
    "    # loss计算\n",
    "    with tf.name_scope(\"loss\") as scope:\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=softmax_linear, labels=targets_reshape, name='xentropy_per_example')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "      \n",
    "    # 训练\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=LearningRate)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "      \n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct = tf.nn.in_top_k(softmax_linear, targets_reshape, 1)\n",
    "        correct = tf.cast(correct, tf.float16)\n",
    "        accuracy = tf.reduce_mean(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4取得batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(Xs, ys, batch_size):\n",
    "    for start in range(0, len(Xs), batch_size):\n",
    "        end = min(start + batch_size, len(Xs))\n",
    "        yield Xs[start:end], ys[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5训练与预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-19T11:05:54.953488: Epoch   0 Batch    0/1223   train_loss = 0.690    accuracy = 0.562\n",
      "2018-02-19T11:07:03.556690: Epoch   0 Batch  200/1223   train_loss = 0.381    accuracy = 0.812\n",
      "2018-02-19T11:08:19.117430: Epoch   0 Batch  400/1223   train_loss = 0.375    accuracy = 0.875\n",
      "2018-02-19T11:09:30.177000: Epoch   0 Batch  600/1223   train_loss = 0.387    accuracy = 0.812\n",
      "2018-02-19T11:10:36.180178: Epoch   0 Batch  800/1223   train_loss = 0.258    accuracy = 0.875\n",
      "2018-02-19T11:11:42.207720: Epoch   0 Batch 1000/1223   train_loss = 0.241    accuracy = 0.812\n",
      "2018-02-19T11:12:46.132675: Epoch   0 Batch 1200/1223   train_loss = 0.056    accuracy = 1.000\n",
      "2018-02-19T11:12:53.265707: Epoch   0 Batch    0/135   cv_loss = 0.029    accuracy = 1.000\n",
      "2018-02-19T11:12:55.838315: Epoch   0 Batch   50/135   cv_loss = 0.028    accuracy = 1.000\n",
      "2018-02-19T11:12:58.392323: Epoch   0 Batch  100/135   cv_loss = 0.011    accuracy = 1.000\n",
      "accurate is: 0.99072\n",
      "2018-02-19T11:13:57.951073: Epoch   1 Batch  177/1223   train_loss = 0.300    accuracy = 0.875\n",
      "2018-02-19T11:15:04.085234: Epoch   1 Batch  377/1223   train_loss = 0.322    accuracy = 0.938\n",
      "2018-02-19T11:16:13.068223: Epoch   1 Batch  577/1223   train_loss = 0.039    accuracy = 1.000\n",
      "2018-02-19T11:17:22.179264: Epoch   1 Batch  777/1223   train_loss = 0.120    accuracy = 0.938\n",
      "2018-02-19T11:18:29.995707: Epoch   1 Batch  977/1223   train_loss = 0.311    accuracy = 0.938\n",
      "2018-02-19T11:19:34.870264: Epoch   1 Batch 1177/1223   train_loss = 0.027    accuracy = 1.000\n",
      "2018-02-19T11:19:49.988501: Epoch   1 Batch   15/135   cv_loss = 0.014    accuracy = 1.000\n",
      "2018-02-19T11:19:52.607720: Epoch   1 Batch   65/135   cv_loss = 0.015    accuracy = 1.000\n",
      "2018-02-19T11:19:55.165327: Epoch   1 Batch  115/135   cv_loss = 0.072    accuracy = 0.938\n",
      "accurate is: 0.99072\n",
      "2018-02-19T11:20:45.653843: Epoch   2 Batch  154/1223   train_loss = 0.045    accuracy = 1.000\n",
      "2018-02-19T11:21:49.654199: Epoch   2 Batch  354/1223   train_loss = 0.183    accuracy = 0.938\n",
      "2018-02-19T11:22:53.630968: Epoch   2 Batch  554/1223   train_loss = 0.130    accuracy = 0.938\n",
      "2018-02-19T11:23:57.836332: Epoch   2 Batch  754/1223   train_loss = 0.011    accuracy = 1.000\n",
      "2018-02-19T11:25:01.636689: Epoch   2 Batch  954/1223   train_loss = 0.026    accuracy = 1.000\n",
      "2018-02-19T11:26:05.586038: Epoch   2 Batch 1154/1223   train_loss = 0.018    accuracy = 1.000\n",
      "2018-02-19T11:26:28.973704: Epoch   2 Batch   30/135   cv_loss = 0.009    accuracy = 1.000\n",
      "2018-02-19T11:26:31.541712: Epoch   2 Batch   80/135   cv_loss = 0.003    accuracy = 1.000\n",
      "2018-02-19T11:26:34.096121: Epoch   2 Batch  130/135   cv_loss = 0.222    accuracy = 0.938\n",
      "accurate is: 0.99268\n",
      "2018-02-19T11:26:38.364731: Batch   30/125 accuracy = 0.938\n",
      "2018-02-19T11:26:40.947339: Batch   80/125 accuracy = 1.000\n",
      "accurate is: 0.99268\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "losses = {'train':[], 'cv':[]}\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    \n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "    accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    \n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "    train_summary_writer = tf.summary.FileWriter(\"runs/train\", sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch_i in range(num_epochs):\n",
    "        train_X, cv_X, train_y, cv_y = train_test_split(train[['fiction']].values,\n",
    "                                                           train['label'].values,\n",
    "                                                           test_size = 0.1,\n",
    "                                                           random_state = 0)\n",
    "        \n",
    "        train_batches = get_batches(train_X, train_y, batch_size)\n",
    "        cv_batches = get_batches(cv_X, cv_y, batch_size)\n",
    "          \n",
    "        #训练的迭代，保存训练损失\n",
    "        for batch_i in range(len(train_X) // batch_size):\n",
    "            x, y = next(train_batches)\n",
    "            \n",
    "            fictions = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                fictions[i] = x.take(0,axis=1)[i]\n",
    "          \n",
    "            feed = {fiction: fictions,\n",
    "                    targets: np.reshape(y, [batch_size, 1]),\n",
    "                    LearningRate: learningrate,\n",
    "                    dropout_keep_prob: 0.5}\n",
    "              \n",
    "            step, train_loss, summaries, _, accurate = sess.run([global_step, loss, train_summary_op, train_op, accuracy], feed)  #cost\n",
    "            losses['train'].append(train_loss)\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "          \n",
    "            if (epoch_i * (len(train_X) // batch_size) + batch_i) % 200 == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}    accuracy = {:.3f}'.format(time_str,\n",
    "                                                                                                         epoch_i,\n",
    "                                                                                                         batch_i,\n",
    "                                                                                                         (len(train_X) // batch_size),\n",
    "                                                                                                         train_loss,\n",
    "                                                                                                         accurate))\n",
    "        accurates = []\n",
    "        for batch_i  in range(len(cv_X) // batch_size):\n",
    "            x, y = next(cv_batches)\n",
    "            \n",
    "            fictions = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                fictions[i] = x.take(0,axis=1)[i]\n",
    "               \n",
    "            feed = {fiction: fictions,\n",
    "                    targets: np.reshape(y, [batch_size, 1]),\n",
    "                    LearningRate: learningrate,\n",
    "                    dropout_keep_prob: 1.0}\n",
    "            \n",
    "            step, cv_loss, summaries, accurate = sess.run([global_step, loss, train_summary_op, accuracy], feed)  #cost\n",
    "            losses['cv'].append(cv_loss)\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "            \n",
    "            accurates.append(accurate)\n",
    "            if (epoch_i * (len(cv_X) // batch_size) + batch_i) % 50 == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   cv_loss = {:.3f}    accuracy = {:.3f}'.format(time_str,\n",
    "                                                                                                      epoch_i,\n",
    "                                                                                                      batch_i,\n",
    "                                                                                                      (len(cv_X) // batch_size),\n",
    "                                                                                                      cv_loss,\n",
    "                                                                                                      accurate))\n",
    "        print('accurate is: ' + str(np.mean(np.array(accurates),axis=0)))\n",
    "    \n",
    "    accurates = []\n",
    "    \n",
    "    test_X = test[['fiction']].values\n",
    "    test_y = test['label'].values\n",
    "    test_batches = get_batches(test_X, test_y, batch_size)\n",
    "    for batch_i  in range(len(test_X) // batch_size):\n",
    "        x, y = next(test_batches)\n",
    "\n",
    "        fictions = np.zeros([batch_size, sentences_size])\n",
    "        for i in range(batch_size):\n",
    "            fictions[i] = x.take(0,axis=1)[i]\n",
    "\n",
    "        feed = {fiction: fictions,\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                LearningRate: learningrate,\n",
    "                dropout_keep_prob: 1.0}\n",
    "\n",
    "        step, accurate = sess.run([global_step, accuracy], feed)  #cost\n",
    "\n",
    "        accurates.append(accurate)\n",
    "        if (epoch_i * (len(test_X) // batch_size) + batch_i) % 50 == 0:\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print('{}: Batch {:>4}/{} accuracy = {:.3f}'.format(time_str,\n",
    "                                                                batch_i,\n",
    "                                                                (len(test_X) // batch_size),\n",
    "                                                                accurate))\n",
    "    print('accurate is: ' + str(np.mean(np.array(accurates),axis=0)))\n",
    "        \n",
    "    ## save model\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、训练效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard中的accurate折线图\n",
    "\n",
    "![accurate折线图](https://github.com/yudake/sundries/blob/master/porn_classify_arrurate.png?raw=true)\n",
    "\n",
    "从图中可以得出，训练后准确率稳定在90%以上，平均准确率在98%。\n",
    "\n",
    "在交叉验证集和测试集上的准确率都在99%。\n",
    "\n",
    "经过多次验证，准确率可以在98%以上。"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
